{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install openai"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2761,"status":"ok","timestamp":1718496579712,"user":{"displayName":"Parth Patel","userId":"15357347181175771297"},"user_tz":-60},"id":"CFTPvkI3Mi7P","outputId":"5fa4e9f0-b7e4-480f-a80c-228a61f0e8d5"},"outputs":[],"source":["# All Imports\n","from openai import OpenAI\n","from datasets import load_dataset\n","import pandas as pd\n","\n","SEED = 2504"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"04T5cKisMi7Q"},"outputs":[],"source":["## Load in the shuffled dataset with given seed\n","dataset = load_dataset(\"rajpurkar/squad_v2\")\n","dataset.set_format(type=\"pandas\")\n","dataset_shuffled = dataset.shuffle(seed=SEED) # shuffle dataset\n","train_dataset = dataset_shuffled[\"train\"] # get the training dataset\n","train_dataset = train_dataset.to_pandas()\n","results_file = \"data/mistral_7b_v2_instruct_squad_2_results.csv\"\n","try:\n","    results_df = pd.read_csv(results_file) # Read in results file that is already created\n","\n","except FileNotFoundError:\n","    # Results file not created. Need to create it\n","\n","    required_columns = [\"ID\", \"topic\", \"question\", \"context\", \"true_answer\", \"llm_answer\"] # These are the chosen columns (Can just append more if needed afterwards)\n","    results_df = pd.DataFrame(columns=required_columns)\n","    results_df.to_csv(results_file, index=False) # write to csv so can store the results"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"ALrEo42sMi7Q"},"outputs":[],"source":["# Point to the local server\n","client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["## Load in results file\n","results_file = \"data/mistral_7b_v2_instruct_squad_2_results.csv\"\n","results_df = pd.read_csv(results_file) # Read in results file that is already created"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"A4Lc7bjwMi7Q"},"outputs":[],"source":["import time\n","\n","def generate_llm_answer_lmstudio(question, context, prompt):\n","    # Construct prompt using the specific question and context for given entry\n","    formatted_prompt = prompt.format(\n","        context=context,\n","        question=question\n","        )\n","\n","    llm_full_output = client.chat.completions.create(\n","        model=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n","        messages=[\n","            {\"role\": \"user\", \"content\": formatted_prompt},\n","        ],\n","        temperature = 0.0, # Temperature set to 0 to help control verbosity\n","    )\n","\n","    # Clean the LLM output. Parse to get the output message\n","    llm_answer = llm_full_output.choices[0].message.content.strip()\n","    return llm_answer\n","\n","def get_llm_output_and_store_results(train_dataset, starting_index, prompt, generate_llm_answer):\n","    batch_size = 25 # Perform a write to csv for every batch processed\n","    batch_results = []\n","    counter = 1\n","    for idx in range(starting_index, len(train_dataset)):\n","        if idx % 5 == 0 :\n","            print(\"Entry : \", idx)\n","\n","        # Extract the required fields from dataset. (ID, question, answer, context)\n","        entry_id = train_dataset.iloc[idx][\"id\"]\n","        topic = train_dataset.iloc[idx][\"title\"]\n","        question = train_dataset.iloc[idx][\"question\"]\n","        context = train_dataset.iloc[idx][\"context\"]\n","        dataset_answer = train_dataset.iloc[idx][\"answers\"][\"text\"]\n","        start_time = time.time()\n","        llm_answer = generate_llm_answer_lmstudio(question, context, prompt) # get the LLM answer\n","        print(\"Time : \", time.time() - start_time)\n","        results_entry = [entry_id, topic, question, context, dataset_answer, llm_answer] # Format as required by output file\n","        batch_results.append(results_entry)\n","\n","        # Write a batch in one go, quicker than writing every time entry done\n","        if len(batch_results) == batch_size :\n","            new_results = pd.DataFrame(batch_results)\n","            new_results.to_csv(results_file, mode=\"a\", index=False, header=False)\n","            print(\"WRITING BATCH\")\n","            batch_results = []\n","            if counter % 20 == 0:\n","                break\n","            counter += 1\n","        print(llm_answer)\n","        # break\n","    \n","def get_start_index(results_df):\n","    ## Get which index to resume training from\n","    return len(results_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N_IlHG08Mi7R"},"outputs":[],"source":["## generate LLM output\n","prompt_template = '''Using only the context that is provided, answer the question as accurately as possible. Provide only the answer to the specific question asked, and keep it as short and concise as possible. Do not include any additional information and do not restate the question or context. If you believe the answer is not provided within the context, then you can say that you don't know.\n","Context : {context}\n","Question : {question}\n","'''\n","starting_index = get_start_index(results_df=results_df)\n","print(\"Starting Index\", starting_index)\n","get_llm_output_and_store_results(\n","    train_dataset=train_dataset,\n","    starting_index=starting_index,\n","    prompt=prompt_template,\n","    generate_llm_answer = generate_llm_answer_lmstudio\n","    )"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"LLM_Virtual_Env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
