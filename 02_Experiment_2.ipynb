{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YBSRJSwKjyfO",
        "E44xRwijf1OP"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This notebook was run in a Google Colab environement.\n",
        "This notebook contains the code for the second experiment\n",
        "'''"
      ],
      "metadata": {
        "id": "W7_uoZTYiDvZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "d06d40b5-7081-4b88-bf49-0322851f7161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThis notebook was run in a Google Colab environement.\\nThis notebook contains the code for the first experiment\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Colab cell\n",
        "# Mount drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/MyDrive/Research Project\")\n",
        "!pip3 install tensorflow\n",
        "!pip3 install gpflow==2.9.1\n",
        "!pip3 install sentence-transformers\n",
        "!pip3 install langchain_text_splitters"
      ],
      "metadata": {
        "id": "g3yx3lRcWPaz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "335f4e31-d7ec-4cfa-c4ac-c6a4546fd3b1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Collecting gpflow==2.9.1\n",
            "  Downloading gpflow-2.9.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting check-shapes>=1.0.0 (from gpflow==2.9.1)\n",
            "  Downloading check_shapes-1.1.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting deprecated (from gpflow==2.9.1)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: multipledispatch>=0.6 in /usr/local/lib/python3.10/dist-packages (from gpflow==2.9.1) (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gpflow==2.9.1) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gpflow==2.9.1) (24.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gpflow==2.9.1) (1.13.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from gpflow==2.9.1) (71.0.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from gpflow==2.9.1) (0.9.0)\n",
            "Requirement already satisfied: tensorflow-probability>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from gpflow==2.9.1) (0.24.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from gpflow==2.9.1) (4.12.2)\n",
            "Requirement already satisfied: tensorflow>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from gpflow==2.9.1) (2.17.0)\n",
            "Collecting dropstackframe>=0.1.0 (from check-shapes>=1.0.0->gpflow==2.9.1)\n",
            "  Downloading dropstackframe-0.1.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting lark<2.0.0,>=1.1.0 (from check-shapes>=1.0.0->gpflow==2.9.1)\n",
            "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (2.32.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (2.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (0.37.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.12.0->gpflow==2.9.1) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.12.0->gpflow==2.9.1) (2.2.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.12.0->gpflow==2.9.1) (0.1.8)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.4.0->gpflow==2.9.1) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=2.4.0->gpflow==2.9.1) (13.8.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=2.4.0->gpflow==2.9.1) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=2.4.0->gpflow==2.9.1) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.4.0->gpflow==2.9.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.4.0->gpflow==2.9.1) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.4.0->gpflow==2.9.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.4.0->gpflow==2.9.1) (2024.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=2.4.0->gpflow==2.9.1) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=2.4.0->gpflow==2.9.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=2.4.0->gpflow==2.9.1) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow>=2.4.0->gpflow==2.9.1) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow>=2.4.0->gpflow==2.9.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow>=2.4.0->gpflow==2.9.1) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow>=2.4.0->gpflow==2.9.1) (0.1.2)\n",
            "Downloading gpflow-2.9.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.6/380.6 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading check_shapes-1.1.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading dropstackframe-0.1.1-py3-none-any.whl (4.6 kB)\n",
            "Downloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lark, dropstackframe, deprecated, check-shapes, gpflow\n",
            "Successfully installed check-shapes-1.1.1 deprecated-1.2.14 dropstackframe-0.1.1 gpflow-2.9.1 lark-1.2.2\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.42.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-3.0.1\n",
            "Collecting langchain_text_splitters\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting langchain-core<0.3.0,>=0.2.10 (from langchain_text_splitters)\n",
            "  Downloading langchain_core-0.2.36-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.10->langchain_text_splitters) (6.0.2)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.10->langchain_text_splitters)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.75 (from langchain-core<0.3.0,>=0.2.10->langchain_text_splitters)\n",
            "  Downloading langsmith-0.1.107-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.10->langchain_text_splitters) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.10->langchain_text_splitters) (2.8.2)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-core<0.3.0,>=0.2.10->langchain_text_splitters)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.10->langchain_text_splitters) (4.12.2)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain_text_splitters)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.10->langchain_text_splitters)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.10->langchain_text_splitters)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.10->langchain_text_splitters) (2.32.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.10->langchain_text_splitters) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.10->langchain_text_splitters) (2.20.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.10->langchain_text_splitters) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.10->langchain_text_splitters) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.10->langchain_text_splitters)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.10->langchain_text_splitters) (3.8)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.10->langchain_text_splitters) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.10->langchain_text_splitters)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.10->langchain_text_splitters) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.10->langchain_text_splitters) (2.0.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.10->langchain_text_splitters) (1.2.2)\n",
            "Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Downloading langchain_core-0.2.36-py3-none-any.whl (395 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langsmith-0.1.107-py3-none-any.whl (150 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, orjson, jsonpointer, h11, jsonpatch, httpcore, httpx, langsmith, langchain-core, langchain_text_splitters\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.2.36 langchain_text_splitters-0.2.2 langsmith-0.1.107 orjson-3.10.7 tenacity-8.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate embeddings"
      ],
      "metadata": {
        "id": "YBSRJSwKjyfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read in data\n",
        "import pandas as pd\n",
        "import ast\n",
        "from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "results_df = pd.read_csv(\"data/mistral_run2_by_topic.csv\")\n",
        "print(results_df.shape)\n",
        "# preprocessing of true answer\n",
        "def preprocess_true_answer(row):\n",
        "    '''\n",
        "    Reads in the true answer column correctly. It is written as a numpy array containing a string, as csv reads in numpy as a string itself.\n",
        "    \"['e.g']\" -> 'e.g'\n",
        "    '''\n",
        "    convert_from_string = ast.literal_eval(row[\"true_answer\"])\n",
        "    if len(convert_from_string) == 0:\n",
        "        convert_from_string = [\"Context does not contain the answer.\"]\n",
        "\n",
        "    return convert_from_string[0]\n",
        "\n",
        "def preprocess_llm_answer(row):\n",
        "    llm_answer = row[\"llm_answer\"]\n",
        "    if \"Context does not contain the answer\" in llm_answer:\n",
        "        llm_answer = \"Context does not contain the answer.\"\n",
        "    return llm_answer\n",
        "\n",
        "results_df[\"true_answer\"] = results_df.apply(preprocess_true_answer, axis=1)\n",
        "results_df[\"llm_answer\"] = results_df.apply(preprocess_llm_answer, axis=1)"
      ],
      "metadata": {
        "id": "Gr7_b1dRr1xg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab69a9f0-dffb-4d3d-b956-dcd2559938d2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11851, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_topics_df(df, num_topics):\n",
        "    '''\n",
        "    This function retrieves (num_topics) many topics that contain the most amount of data points. (n-largest)\n",
        "    '''\n",
        "    counts_by_topic = df[\"topic\"].value_counts()\n",
        "    top_topics = counts_by_topic.nlargest(num_topics).index\n",
        "    top_topics_df = df[df[\"topic\"].isin(top_topics)]\n",
        "    return top_topics_df.reset_index(drop=True)\n",
        "\n",
        "results_df = get_top_topics_df(results_df, num_topics=11) # get 11 topics\n",
        "results_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y42Ke4eKj9rm",
        "outputId": "e90d93c7-11c5-4885-ddba-53bc508fc7ed"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7325, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove long question+contexts\n",
        "\n",
        "text_splitter = SentenceTransformersTokenTextSplitter(model_name=\"sentence-transformers/all-mpnet-base-v2\") # tokenise according to mpnet-base ST\n",
        "all_questions = results_df[\"question\"].tolist()\n",
        "all_contexts = results_df[\"context\"].tolist()\n",
        "all_question_context_combined = [f\"{question} {context}\" for question,context in zip(all_questions, all_contexts)] # string strategy\n",
        "\n",
        "num_tokens_question_context_combined = [text_splitter.count_tokens(text=entry) - 2 for entry in all_question_context_combined] # count tokens for string strategy\n",
        "indices_too_long = [index for index, token_count in enumerate(num_tokens_question_context_combined) if token_count > 382] # indices that have a token length of > 384 and hence will be truncated\n",
        "\n",
        "results_df = results_df.drop(indices_too_long).reset_index(drop=True)\n",
        "results_df.shape"
      ],
      "metadata": {
        "id": "dhTKAdoEt7EI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = SentenceTransformersTokenTextSplitter(model_name=\"sentence-transformers/all-distilroberta-v1\") # tokenise according to distilroberta ST\n",
        "all_questions = results_df[\"question\"].tolist()\n",
        "all_contexts = results_df[\"context\"].tolist()\n",
        "all_question_context_combined = [f\"{question} {context}\" for question,context in zip(all_questions, all_contexts)] # string strategy\n",
        "\n",
        "num_tokens_question_context_combined = [text_splitter.count_tokens(text=entry) - 2 for entry in all_question_context_combined] # count tokens for string strategy\n",
        "indices_too_long = [index for index, token_count in enumerate(num_tokens_question_context_combined) if token_count > 510] # indices that have a token length of > 512 and hence will be truncated\n",
        "results_df = results_df.drop(indices_too_long).reset_index(drop=True)\n",
        "\n",
        "print(results_df.shape)"
      ],
      "metadata": {
        "id": "w5dEuQKRt8An"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(sentences_list, model):\n",
        "    '''\n",
        "    This function produces an embedding for a list of sentences that is provided to the function.\n",
        "    '''\n",
        "    pool = model.start_multi_process_pool()\n",
        "    embeddings = model.encode_multi_process(sentences_list, pool)\n",
        "    model.stop_multi_process_pool(pool)\n",
        "    return embeddings.tolist()\n",
        "\n",
        "## Create input embeddings\n",
        "all_questions = results_df[\"question\"].tolist()\n",
        "all_contexts = results_df[\"context\"].tolist()\n",
        "all_question_context_combined = [f\"{question} {context}\" for question,context in zip(all_questions, all_contexts)]\n",
        "\n",
        "# load sentence transformer model\n",
        "model_list = {\n",
        "    \"mpnet\" : SentenceTransformer(\"all-mpnet-base-v2\"),\n",
        "    \"distill_roberta\" : SentenceTransformer(\"all-distilroberta-v1\")\n",
        "}\n",
        "\n",
        "for input_representation in [\"mpnet\", \"distill_roberta\"]:\n",
        "    print(\"Input \", input_representation)\n",
        "    # string strat\n",
        "    results_df[f\"question_and_context_{input_representation}\"] = get_embedding(all_question_context_combined, model_list[input_representation])\n",
        "\n",
        "    # concat strat\n",
        "    results_df[f\"question_{input_representation}\"] = get_embedding(all_questions, model_list[input_representation])\n",
        "    results_df[f\"context_{input_representation}\"] = get_embedding(all_contexts, model_list[input_representation])"
      ],
      "metadata": {
        "id": "VbI0TUiBvwGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_similarity_score_column(column1, column2, model):\n",
        "    '''\n",
        "    Calculate the similarity scores (pairwise) between the first and second column. (dataset answer and llm answer)\n",
        "    column1 and column2 is list of strings.\n",
        "    '''\n",
        "    column1_embedding = model.encode(column1)\n",
        "    column2_embedding = model.encode(column2)\n",
        "    similarity_scores = model.similarity_pairwise(column1_embedding, column2_embedding)\n",
        "    return similarity_scores.numpy()\n",
        "\n",
        "# calculate targets\n",
        "all_true_answers_list = results_df[\"true_answer\"].tolist()\n",
        "all_llm_answers_list = results_df[\"llm_answer\"].tolist()\n",
        "\n",
        "for target_representation in [\"mpnet\", \"distill_roberta\"]:\n",
        "    results_df[f\"{target_representation}_score\"] = calc_similarity_score_column(all_true_answers_list, all_llm_answers_list, model_list[target_representation])\n"
      ],
      "metadata": {
        "id": "8mlofiVlwrmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df.to_csv(\"data/selected_topics_embeddings_multiple_inputs.csv\", index=False) # save to csv"
      ],
      "metadata": {
        "id": "CYxlwSSWz-0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fit GP"
      ],
      "metadata": {
        "id": "btBmMn5KkWFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from gpflow.models import GPR\n",
        "from gpflow.kernels import RationalQuadratic\n",
        "from gpflow.mean_functions import Zero\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import pickle\n",
        "import scipy.stats as stats\n",
        "import gpflow\n",
        "\n",
        "SEED = 2504\n",
        "\n",
        "# Read in generated embeddings and targets\n",
        "results_df = pd.read_csv(\"data/selected_topics_embeddings_multiple_inputs.csv\") # Dataset for experiment 2\n",
        "\n",
        "\n",
        "## correctly read in saved embeddings\n",
        "# mpnet embeddings\n",
        "results_df[\"question_mpnet\"] = [ast.literal_eval(x) for x in results_df[\"question_mpnet\"]]\n",
        "results_df[\"context_mpnet\"] = [ast.literal_eval(x) for x in results_df[\"context_mpnet\"]]\n",
        "# results_df[\"question_and_context_mpnet\"] = [ast.literal_eval(x) for x in results_df[\"question_and_context_mpnet\"]]\n",
        "\n",
        "# # distill_roberta_embeddings\n",
        "# results_df[\"question_distill_roberta\"] = [ast.literal_eval(x) for x in results_df[\"question_distill_roberta\"]]\n",
        "# results_df[\"context_distill_roberta\"] = [ast.literal_eval(x) for x in results_df[\"context_distill_roberta\"]]\n",
        "# results_df[\"question_and_context_distill_roberta\"] = [ast.literal_eval(x) for x in results_df[\"question_and_context_distill_roberta\"]]\n",
        "\n",
        "# create OHE vector\n",
        "ohe_topics = pd.get_dummies(results_df['topic'], prefix='topic', dtype=\"int\")\n",
        "results_df[\"topic_OHE\"] = ohe_topics.values.tolist()"
      ],
      "metadata": {
        "id": "XPoevJeWpXj3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preds_vs_truth_plot(train_true_vals, train_pred_vals, test_true_vals, test_pred_vals, file_name=None):\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12,8))\n",
        "\n",
        "    ## plot train data\n",
        "    axs[0].scatter(train_true_vals, train_pred_vals, alpha=0.4, label=f\"Train data\", color=\"blue\") # plot train points\n",
        "\n",
        "    # plot line of perfec predictions for comparison\n",
        "    min_value_train = min(np.min(train_true_vals), np.min(train_pred_vals))\n",
        "    max_value_train = max(np.max(train_true_vals), np.max(train_pred_vals))\n",
        "    axs[0].plot(np.arange(min_value_train, max_value_train, step=0.001),\n",
        "                np.arange(min_value_train, max_value_train, step=0.001),\n",
        "                label=\"Perfect predictions\", color=\"red\")\n",
        "    axs[0].set_xlabel(\"True score\")\n",
        "    axs[0].set_ylabel(\"Predicted score\")\n",
        "    axs[0].legend(fontsize=\"x-small\")\n",
        "    axs[0].set_title(\"Train\", fontsize=10)\n",
        "\n",
        "    ## plot test data\n",
        "    axs[1].scatter(test_true_vals, test_pred_vals, alpha=0.4, label=f\"Train data\", color=\"blue\") # plot test points\n",
        "\n",
        "    # plot line of perfect predictions for comparison\n",
        "    min_val_test = min(np.min(test_true_vals), np.min(test_pred_vals))\n",
        "    max_val_test = max(np.max(test_true_vals), np.max(test_pred_vals))\n",
        "\n",
        "    axs[1].plot(np.arange(min_val_test, max_val_test, step=0.001),\n",
        "                np.arange(min_val_test, max_val_test, step=0.001),\n",
        "                label=\"Perfect predictions\", color=\"red\")\n",
        "    axs[1].set_xlabel(\"True score\")\n",
        "    axs[1].set_ylabel(\"Predicted score\")\n",
        "    axs[1].legend(fontsize=\"x-small\")\n",
        "    axs[1].set_title(\"Test\", fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if file_name is not None:\n",
        "        plt.savefig(file_name)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "wQGMJN89p4p3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gp_model_init(x_train, y_train, lengthscale, alpha, noise):\n",
        "    # Initilaise GP with data and hyperparameters provided\n",
        "    return GPR(\n",
        "        data=(x_train, y_train),\n",
        "        kernel= RationalQuadratic(lengthscales=lengthscale, alpha=alpha),\n",
        "        mean_function=Zero(), # zero mean function\n",
        "        noise_variance=noise\n",
        "    )\n",
        "\n",
        "def fit_gp_function(results_df, all_gp_inputs, target, save_folder, data_category, condition=None, num_restarts=20):\n",
        "    best_model, best_log_marginal_likelihood_val = None, -np.inf\n",
        "\n",
        "    if condition is not None:\n",
        "        # Answerable or unanswerble data\n",
        "        all_targets = results_df[condition][target].to_numpy().reshape(-1, 1)\n",
        "        title_col = results_df[condition][\"topic\"]\n",
        "\n",
        "    else:\n",
        "        # full data\n",
        "        all_targets = results_df[target].to_numpy().reshape(-1, 1)\n",
        "        title_col = results_df[\"topic\"]\n",
        "\n",
        "    # 80-20 trian-test split\n",
        "    x_train, x_test, y_train, y_test= train_test_split(all_gp_inputs, all_targets, test_size=0.20,\n",
        "                                                       stratify=title_col, random_state=SEED)\n",
        "\n",
        "    # scale\n",
        "    x_scaler = StandardScaler()\n",
        "    x_train_scaled = x_scaler.fit_transform(x_train)\n",
        "    x_test_scaled = x_scaler.transform(x_test)\n",
        "\n",
        "    y_scaler = StandardScaler()\n",
        "    y_train_scaled = y_scaler.fit_transform(y_train)\n",
        "\n",
        "    for i in range(num_restarts):\n",
        "        print(f\"Restart {i}\")\n",
        "        # generate random initialisation vals\n",
        "        lengthscale_init = stats.loguniform.rvs(0.01, 100)\n",
        "        alpha_init = stats.loguniform.rvs(0.01, 100)\n",
        "        noise_init = stats.loguniform.rvs(0.01, 100)\n",
        "\n",
        "        print(lengthscale_init, alpha_init, noise_init)\n",
        "\n",
        "        # initialise Gp\n",
        "        model = gp_model_init(x_train_scaled, y_train_scaled, lengthscale_init, alpha_init, noise_init)\n",
        "        opt = gpflow.optimizers.Scipy()\n",
        "        opt.minimize(model.training_loss, model.trainable_variables) # perform optimization\n",
        "        log_marginal_likelihood_val = model.log_marginal_likelihood().numpy() # return log marginal likelihood\n",
        "        print(log_marginal_likelihood_val)\n",
        "        # update if higher log marginal likelihood found\n",
        "        if log_marginal_likelihood_val > best_log_marginal_likelihood_val:\n",
        "            best_log_marginal_likelihood_val = log_marginal_likelihood_val\n",
        "            best_model = model\n",
        "\n",
        "    mean_train, _ = best_model.predict_y(x_train_scaled)\n",
        "    mean_test, _ = best_model.predict_y(x_test_scaled)\n",
        "\n",
        "    mean_test_rev = y_scaler.inverse_transform(mean_test.numpy())\n",
        "\n",
        "    # calculate eval metrics\n",
        "    mae = mean_absolute_error(y_test.squeeze(), mean_test_rev)\n",
        "    rmse = mean_squared_error(y_test.squeeze(), mean_test_rev, squared=False)\n",
        "    r2 = r2_score(y_test.squeeze(), mean_test_rev)\n",
        "\n",
        "    # preds_vs_truth_plot(y_scaler.inverse_transform(y_train_scaled), y_scaler.inverse_transform(mean_train.numpy()),\n",
        "    #                     y_test, mean_test_rev)\n",
        "\n",
        "    print(f\"Best Log Likelihood: {best_log_marginal_likelihood_val}\")\n",
        "    print(f\"MAE : {mae}\")\n",
        "    print(f\"RMSE : {rmse}\")\n",
        "    print(f\"R2 : {r2}\")\n",
        "\n",
        "    # save preds and metrics\n",
        "    train_results_df = pd.DataFrame({\n",
        "        \"true\" : y_scaler.inverse_transform(y_train_scaled).squeeze(),\n",
        "        \"prediction\" : y_scaler.inverse_transform(mean_train.numpy()).squeeze()\n",
        "    })\n",
        "\n",
        "    test_results_df = pd.DataFrame({\n",
        "        \"true\" : y_test.squeeze(),\n",
        "        \"prediction\" : mean_test_rev.squeeze()\n",
        "    })\n",
        "\n",
        "    # train_results_df.to_csv(f\"{save_folder}_train_vals.csv\", index=False)\n",
        "    # test_results_df.to_csv(f\"{save_folder}_test_vals.csv\", index=False)\n",
        "\n",
        "    test_metrics_df = pd.DataFrame({\n",
        "        \"mae\" : [mae],\n",
        "        \"rmse\" : [rmse],\n",
        "        \"r2\" : [r2]\n",
        "    })\n",
        "\n",
        "    # test_metrics_df.to_csv(f\"{save_folder}_test_metrics.csv\", index=False)\n",
        "    print(test_metrics_df)\n",
        "\n",
        "    # save optimized hyperparam of best model\n",
        "    # param_dict = gpflow.utilities.parameter_dict(best_model)\n",
        "    # with open(f\"{save_folder}_param_dict.pkl\", \"wb\") as f:\n",
        "    #     pickle.dump(param_dict, f)\n"
      ],
      "metadata": {
        "id": "pEqFk7hCr4Pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_input_representation = [\"mpnet\", \"distill_roberta\"]\n",
        "all_targets = [\"mpnet_score\", \"distill_roberta_score\"]\n",
        "\n",
        "# Full\n",
        "for input_representation in all_input_representation:\n",
        "    print(input_representation)\n",
        "    for target_representation in all_targets:\n",
        "        # String, no OHE\n",
        "        all_gp_inputs = np.array(results_df[f\"question_and_context_{input_representation}\"].tolist())\n",
        "        fit_gp_function(results_df,\n",
        "                all_gp_inputs,\n",
        "                target = target_representation,\n",
        "                save_folder=f\"experiment_2/{target_representation}/full_input_{input_representation}_string\",\n",
        "                data_category=\"full\",\n",
        "                condition=None,\n",
        "                )\n",
        "\n",
        "        # String, OHE\n",
        "        all_gp_inputs = np.hstack([\n",
        "            np.array(results_df[f\"question_and_context_{input_representation}\"].tolist()),\n",
        "            np.array(results_df[\"topic_OHE\"].tolist())\n",
        "            ])\n",
        "\n",
        "        fit_gp_function(results_df,\n",
        "                all_gp_inputs,\n",
        "                target = target_representation,\n",
        "                save_folder=f\"experiment_2/{target_representation}/ohe/full_input_{input_representation}_string\",\n",
        "                data_category=\"full\",\n",
        "                condition=None,\n",
        "                )\n",
        "\n",
        "        # Concat, no ohe\n",
        "        all_gp_inputs = np.hstack([\n",
        "            np.array(results_df[f\"question_{input_representation}\"].tolist()),\n",
        "            np.array(results_df[f\"context_{input_representation}\"].tolist()),\n",
        "            ])\n",
        "\n",
        "        fit_gp_function(\n",
        "            results_df,\n",
        "            all_gp_inputs,\n",
        "            target = target_representation,\n",
        "            save_folder=f\"experiment_2/{target_representation}/full_input_{input_representation}_concat\",\n",
        "            data_category=\"full\",\n",
        "            condition=None,\n",
        "                )\n",
        "\n",
        "        # concat OHE\n",
        "        all_gp_inputs = np.hstack([\n",
        "            np.array(results_df[f\"question_{input_representation}\"].tolist()),\n",
        "            np.array(results_df[f\"context_{input_representation}\"].tolist()),\n",
        "            np.array(results_df[\"topic_OHE\"].tolist())\n",
        "            ])\n",
        "        fit_gp_function(\n",
        "            results_df,\n",
        "            all_gp_inputs,\n",
        "            target = target_representation,\n",
        "            save_folder=f\"experiment_2/{target_representation}/ohe/full_input_{input_representation}_concat\",\n",
        "            data_category=\"full\",\n",
        "            condition=None,\n",
        "                )"
      ],
      "metadata": {
        "id": "z0ZkbIwZmkeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_input_representation = [\"mpnet\", \"distill_roberta\"]\n",
        "all_targets = [\"mpnet_score\", \"distill_roberta_score\"]\n",
        "\n",
        "# Answerable/Unanswerable, swap out condition\n",
        "for input_representation in all_input_representation:\n",
        "    print(input_representation)\n",
        "    for target_representation in all_targets:\n",
        "        # String, no OHE\n",
        "        all_gp_inputs = np.array(results_df[results_df[\"true_answer\"] != \"Context does not contain the answer.\"][f\"question_and_context_{input_representation}\"].tolist())\n",
        "        fit_gp_function(\n",
        "            results_df,\n",
        "            all_gp_inputs,\n",
        "            target = target_representation,\n",
        "            save_folder=f\"experiment_2/{target_representation}/answerable_input_{input_representation}_string\",\n",
        "            data_category=\"answerable\",\n",
        "            condition= results_df[\"true_answer\"] != \"Context does not contain the answer.\",\n",
        "                )\n",
        "\n",
        "        # String, OHE\n",
        "        all_gp_inputs = np.hstack([\n",
        "            np.array(results_df[results_df[\"true_answer\"] != \"Context does not contain the answer.\"][f\"question_and_context_{input_representation}\"].tolist()),\n",
        "            np.array(results_df[results_df[\"true_answer\"] != \"Context does not contain the answer.\"][\"topic_OHE\"].tolist())\n",
        "            ])\n",
        "\n",
        "        fit_gp_function(results_df,\n",
        "                all_gp_inputs,\n",
        "                target = target_representation,\n",
        "                save_folder=f\"experiment_2/{target_representation}/ohe/answerable_input_{input_representation}_string\",\n",
        "                data_category=\"answerable\",\n",
        "                condition= results_df[\"true_answer\"] != \"Context does not contain the answer.\",\n",
        "                )\n",
        "\n",
        "        # Concat, no ohe\n",
        "        all_gp_inputs = np.hstack([\n",
        "            np.array(results_df[results_df[\"true_answer\"] != \"Context does not contain the answer.\"][f\"question_{input_representation}\"].tolist()),\n",
        "            np.array(results_df[results_df[\"true_answer\"] != \"Context does not contain the answer.\"][f\"context_{input_representation}\"].tolist()),\n",
        "            ])\n",
        "\n",
        "        fit_gp_function(results_df,\n",
        "                all_gp_inputs,\n",
        "                target = target_representation,\n",
        "                save_folder=f\"experiment_2/{target_representation}/answerable_input_{input_representation}_concat\",\n",
        "                data_category=\"answerable\",\n",
        "                condition= results_df[\"true_answer\"] != \"Context does not contain the answer.\",\n",
        "                )\n",
        "\n",
        "        # concat OHE\n",
        "        all_gp_inputs = np.hstack([\n",
        "            np.array(results_df[results_df[\"true_answer\"] != \"Context does not contain the answer.\"][f\"question_{input_representation}\"].tolist()),\n",
        "            np.array(results_df[results_df[\"true_answer\"] != \"Context does not contain the answer.\"][f\"context_{input_representation}\"].tolist()),\n",
        "            np.array(results_df[results_df[\"true_answer\"] != \"Context does not contain the answer.\"][\"topic_OHE\"].tolist())\n",
        "            ])\n",
        "        fit_gp_function(results_df,\n",
        "                all_gp_inputs,\n",
        "                target = target_representation,\n",
        "                save_folder=f\"experiment_2/{target_representation}/ohe/answerable_input_{input_representation}_concat\",\n",
        "                data_category=\"answerable\",\n",
        "                condition= results_df[\"true_answer\"] != \"Context does not contain the answer.\",\n",
        "                )"
      ],
      "metadata": {
        "id": "hUW8F_benL2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dummy regressor (predicts mean)\n",
        "from sklearn.dummy import DummyRegressor\n",
        "input_representation = \"mpnet\"\n",
        "target_representation = \"distill_roberta_score\"\n",
        "all_targets = results_df[target_representation].to_numpy().reshape(-1, 1)\n",
        "title_col = results_df[\"topic\"]\n",
        "all_gp_inputs = np.hstack([\n",
        "            np.array(results_df[f\"question_{input_representation}\"].tolist()),\n",
        "            np.array(results_df[f\"context_{input_representation}\"].tolist()),\n",
        "            ])\n",
        "\n",
        "# 80-20 train-test split\n",
        "x_train, x_test, y_train, y_test= train_test_split(all_gp_inputs, all_targets, test_size=0.20,\n",
        "                                                   stratify=title_col, random_state=SEED)\n",
        "\n",
        "dummy_reg = DummyRegressor(strategy=\"mean\")\n",
        "dummy_reg.fit(x_train, y_train.ravel())\n",
        "\n",
        "mean_test = dummy_reg.predict(x_test).reshape(-1,1)\n",
        "\n",
        "# calculate eval metrics\n",
        "mae = mean_absolute_error(y_test.squeeze(), mean_test)\n",
        "rmse = mean_squared_error(y_test.squeeze(), mean_test, squared=False)\n",
        "r2 = r2_score(y_test.squeeze(), mean_test)\n",
        "\n",
        "print(f\"MAE : {mae}\")\n",
        "print(f\"RMSE : {rmse}\")\n",
        "print(f\"R2 : {r2}\")"
      ],
      "metadata": {
        "id": "ffMAH7Vj_N7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA mini study\n"
      ],
      "metadata": {
        "id": "cMaW3IGQfz78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def gp_model_init(x_train, y_train, lengthscale, alpha, noise):\n",
        "    # Initilaise GP with data and hyperparameters provided\n",
        "    return GPR(\n",
        "        data=(x_train, y_train),\n",
        "        kernel= RationalQuadratic(lengthscales=lengthscale, alpha=alpha),\n",
        "        mean_function=Zero(), # zero mean function\n",
        "        noise_variance=noise\n",
        "    )\n",
        "\n",
        "def fit_gp_function(results_df, all_gp_inputs, target, num_restarts=4, pca_study=None):\n",
        "    best_model, best_log_marginal_likelihood_val = None, -np.inf\n",
        "    print(\"PCA\")\n",
        "    # full data\n",
        "    all_targets = results_df[target].to_numpy().reshape(-1, 1)\n",
        "    title_col = results_df[\"topic\"]\n",
        "\n",
        "    # 80-20 trian-test split\n",
        "    x_train, x_test, y_train, y_test= train_test_split(all_gp_inputs, all_targets, test_size=0.20, stratify=title_col, random_state=SEED)\n",
        "\n",
        "    # scale\n",
        "    x_scaler = StandardScaler()\n",
        "    x_train_scaled = x_scaler.fit_transform(x_train)\n",
        "    x_test_scaled = x_scaler.transform(x_test)\n",
        "\n",
        "    y_scaler = StandardScaler()\n",
        "    y_train_scaled = y_scaler.fit_transform(y_train)\n",
        "\n",
        "    pca = PCA(n_components=pca_study)\n",
        "    x_train_scaled = pca.fit_transform(x_train_scaled)\n",
        "    x_test_scaled = pca.transform(x_test_scaled)\n",
        "\n",
        "    for i in range(num_restarts):\n",
        "        print(f\"Restart {i}\")\n",
        "        # generate random initialisation vals\n",
        "        lengthscale_init = stats.loguniform.rvs(0.01, 100)\n",
        "        alpha_init = stats.loguniform.rvs(0.01, 100)\n",
        "        noise_init = stats.loguniform.rvs(0.01, 100)\n",
        "\n",
        "        # print(lengthscale_init, alpha_init, noise_init)\n",
        "\n",
        "        # initialise Gp\n",
        "        model = gp_model_init(x_train_scaled, y_train_scaled, lengthscale_init, alpha_init, noise_init)\n",
        "        opt = gpflow.optimizers.Scipy()\n",
        "        opt.minimize(model.training_loss, model.trainable_variables) # perform optimization\n",
        "        log_marginal_likelihood_val = model.log_marginal_likelihood().numpy() # return log marginal likelihood\n",
        "        print(log_marginal_likelihood_val)\n",
        "\n",
        "        # update if higher log marginal likelihood found\n",
        "        if log_marginal_likelihood_val > best_log_marginal_likelihood_val:\n",
        "            best_log_marginal_likelihood_val = log_marginal_likelihood_val\n",
        "            best_model = model\n",
        "\n",
        "    mean_train, _ = best_model.predict_y(x_train_scaled)\n",
        "    mean_test, _ = best_model.predict_y(x_test_scaled)\n",
        "\n",
        "    mean_test_rev = y_scaler.inverse_transform(mean_test.numpy())\n",
        "\n",
        "    # calculate eval metrics\n",
        "    mae = mean_absolute_error(y_test.squeeze(), mean_test_rev)\n",
        "    rmse = mean_squared_error(y_test.squeeze(), mean_test_rev, squared=False)\n",
        "    r2 = r2_score(y_test.squeeze(), mean_test_rev)\n",
        "\n",
        "    return rmse\n"
      ],
      "metadata": {
        "id": "qnJgoIWUpqHL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_dimensions = [1, 200, 400, 600, 800, 1000, 1200, 1400, 1547]\n",
        "rmse_results = []\n",
        "input_representation = \"mpnet\"\n",
        "all_gp_inputs = np.hstack([np.array(results_df[f\"question_{input_representation}\"].tolist()),\n",
        "                           np.array(results_df[f\"context_{input_representation}\"].tolist()),\n",
        "                           np.array(results_df[\"topic_OHE\"].tolist())\n",
        "])\n",
        "\n",
        "for dim in pca_dimensions:\n",
        "    rmse = fit_gp_function(results_df,\n",
        "                           all_gp_inputs,\n",
        "                           target=\"mpnet_score\",\n",
        "                           num_restarts=5,\n",
        "                           pca_study=dim)\n",
        "    rmse_results.append(rmse)\n",
        "\n",
        "df_res = pd.DataFrame({\n",
        "    \"PCA_Dimensions\" : pca_dimesions,\n",
        "    \"RMSE\" : rmse_results\n",
        "})\n",
        "df_res.to_csv(\"pca_dimensions_vs_rmse.csv\", index=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(pca_dimensions, rmse_results, marker='o')\n",
        "plt.title('RMSE for varying input dimensions using PCA')\n",
        "plt.xlabel('Number of input dimensions')\n",
        "plt.ylabel('RMSE')\n",
        "plt.grid(True)\n",
        "plt.savefig(\"PCA_DIM_plot.pdf\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SpQ8QzD7pY3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Error Analysis"
      ],
      "metadata": {
        "id": "SXcB3T_WSZRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "SEED = 2504\n",
        "\n",
        "input_representation = \"mpnet\"\n",
        "target_representation = \"mpnet_score\"\n",
        "\n",
        "# read in predictions and data\n",
        "train_preds = pd.read_csv(f\"experiment_2/{target_representation}/ohe/full_input_{input_representation}_concat_train_vals.csv\")\n",
        "train_preds = pd.read_csv(f\"experiment_2/{target_representation}/ohe/full_input_{input_representation}_concat_test_vals.csv\")\n",
        "results_df = pd.read_csv(\"data/selected_topics_embeddings_multiple_inputs.csv\") # dataset for experiment 2 (11 topic dataset)\n",
        "\n",
        "# read in numpy arrays correctly from csv\n",
        "results_df[\"question_mpnet\"] = [ast.literal_eval(x) for x in results_df[\"question_mpnet\"]]\n",
        "results_df[\"context_mpnet\"] = [ast.literal_eval(x) for x in results_df[\"context_mpnet\"]]\n",
        "\n",
        "ohe_topics = pd.get_dummies(results_df[\"topic\"], prefix=\"topic\", dtype=\"int\") # create OHE\n",
        "results_df[\"topic_OHE\"] = ohe_topics.values.tolist() # OHE vector"
      ],
      "metadata": {
        "id": "pE9HSFKjboRv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input and target setup\n",
        "all_gp_inputs = np.hstack([np.array(results_df[f\"question_{input_representation}\"].tolist()),\n",
        "                           np.array(results_df[f\"context_{input_representation}\"].tolist()),\n",
        "                           np.array(results_df[\"topic_OHE\"].tolist())\n",
        "])\n",
        "all_gp_targets = results_df[target_representation].to_numpy().reshape(-1, 1)\n",
        "topic_col = results_df[\"topic\"]\n",
        "\n",
        "# create train-test split to get test set\n",
        "x_train, x_test, y_train, y_test, train_indices, test_indices = train_test_split(all_gp_inputs, all_gp_targets, np.arange(len(all_gp_targets)),\n",
        "                                                    stratify=topic_col, test_size=0.2, random_state=SEED)"
      ],
      "metadata": {
        "id": "FadQ_Q-ncJLm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_errors = train_preds[\"true\"].to_numpy() - train_preds[\"prediction\"].to_numpy()\n",
        "\n",
        "# create dataframe with residuals\n",
        "# replace commands help provide clean legends\n",
        "test_res_df = pd.DataFrame({\n",
        "    \"topic\" : [title_entry.replace(\",\", \"\").replace(\"_\", \" \") for title_entry in topic_col.iloc[test_indices]],\n",
        "    \"error\" : test_errors,\n",
        "    \"absolute_error\" : np.abs(test_errors),\n",
        "    \"squared_error\" : np.square(test_errors),\n",
        "})\n"
      ],
      "metadata": {
        "id": "zn_qj7j-c9qD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Study 1\n",
        "# get mse and mae by topic\n",
        "test_rmse_topic = np.sqrt(test_res_df.groupby(\"topic\").mean()[\"squared_error\"])\n",
        "test_counts = test_res_df.groupby(\"topic\").size() # get num of datapoints per topic"
      ],
      "metadata": {
        "id": "iHl7wfpte_-S"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_rmse_best = test_rmse_topic.idxmin()\n",
        "test_rmse_worst = test_rmse_topic.idxmax()\n",
        "\n",
        "# create plot\n",
        "plt.figure(figsize=(7, 7))\n",
        "\n",
        "bar_colours = [\"green\" if topic == test_rmse_best else (\"red\" if topic == test_rmse_worst else \"grey\") for topic in test_rmse_topic.index] # color worst in red, best in green, all others in grey\n",
        "bar_chart = plt.bar(test_rmse_topic.index, test_rmse_topic.values, color=bar_colours)\n",
        "plt.xlabel(\"Topic\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.xticks(test_rmse_topic.index, rotation=90, fontsize=7)\n",
        "# add topic count\n",
        "for bar, count in zip(bar_chart, test_counts.values):\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),\n",
        "            f\"{count}\", ha='center', va='bottom', fontsize=7)\n",
        "\n",
        "plt.title(\"Test RMSE breakdown by topic\")\n",
        "plt.tight_layout()\n",
        "# plt.savefig(\"error_analysis_rmse_topic_breakdown.pdf\")"
      ],
      "metadata": {
        "id": "SMNOWgvnjn2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "test_topics = test_res_df[\"topic\"]\n",
        "topic_colours = {topic:colour for topic,colour in zip(test_topics.unique(), sns.color_palette(\"tab20\", 11))} # assign topic colour\n",
        "\n",
        "test_embeddings_viz = PCA(n_components=2).fit_transform(x_test)\n",
        "#\n",
        "plt.figure(figsize=(8, 5))\n",
        "\n",
        "for topic in test_topics.unique():\n",
        "    topic_indices = np.where(test_topics == topic)[0] # get topic indices\n",
        "    plt.scatter(test_embeddings_viz[topic_indices, 0], test_embeddings_viz[topic_indices, 1], label=topic, color=topic_colours[topic]) # plot 2d points coloured by topic\n",
        "\n",
        "plt.xlabel(\"Principal component 1\")\n",
        "plt.ylabel(\"Principal component 2\")\n",
        "plt.legend(loc=\"best\", bbox_to_anchor=(1, 1))\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.title(\"2D PCA visualisation of test embeddings by topic\")\n",
        "plt.savefig(\"2d_pca_visualisation.pdf\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O_v2trOPrsoT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}