{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "W7_uoZTYiDvZ",
        "outputId": "d06d40b5-7081-4b88-bf49-0322851f7161"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nThis notebook was run in a Google Colab environement.\\nThis notebook contains the code for the first experiment\\n'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "This notebook was run in a Google Colab environement.\n",
        "This notebook contains the code for the first experiment\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3yx3lRcWPaz",
        "outputId": "8a33231c-9571-40cb-c059-8e97628e8f46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Collecting gpflow==2.9.1\n",
            "  Downloading gpflow-2.9.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting check-shapes>=1.0.0 (from gpflow==2.9.1)\n",
            "  Downloading check_shapes-1.1.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting deprecated (from gpflow==2.9.1)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: multipledispatch>=0.6 in /usr/local/lib/python3.10/dist-packages (from gpflow==2.9.1) (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gpflow==2.9.1) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gpflow==2.9.1) (24.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gpflow==2.9.1) (1.13.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from gpflow==2.9.1) (71.0.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from gpflow==2.9.1) (0.9.0)\n",
            "Requirement already satisfied: tensorflow-probability>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from gpflow==2.9.1) (0.24.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from gpflow==2.9.1) (4.12.2)\n",
            "Requirement already satisfied: tensorflow>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from gpflow==2.9.1) (2.17.0)\n",
            "Collecting dropstackframe>=0.1.0 (from check-shapes>=1.0.0->gpflow==2.9.1)\n",
            "  Downloading dropstackframe-0.1.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting lark<2.0.0,>=1.1.0 (from check-shapes>=1.0.0->gpflow==2.9.1)\n",
            "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (2.32.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (2.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.0->gpflow==2.9.1) (0.37.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.12.0->gpflow==2.9.1) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.12.0->gpflow==2.9.1) (2.2.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.12.0->gpflow==2.9.1) (0.1.8)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.4.0->gpflow==2.9.1) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=2.4.0->gpflow==2.9.1) (13.8.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=2.4.0->gpflow==2.9.1) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=2.4.0->gpflow==2.9.1) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.4.0->gpflow==2.9.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.4.0->gpflow==2.9.1) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.4.0->gpflow==2.9.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.4.0->gpflow==2.9.1) (2024.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=2.4.0->gpflow==2.9.1) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=2.4.0->gpflow==2.9.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=2.4.0->gpflow==2.9.1) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow>=2.4.0->gpflow==2.9.1) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow>=2.4.0->gpflow==2.9.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow>=2.4.0->gpflow==2.9.1) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow>=2.4.0->gpflow==2.9.1) (0.1.2)\n",
            "Downloading gpflow-2.9.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.6/380.6 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading check_shapes-1.1.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading dropstackframe-0.1.1-py3-none-any.whl (4.6 kB)\n",
            "Downloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lark, dropstackframe, deprecated, check-shapes, gpflow\n",
            "Successfully installed check-shapes-1.1.1 deprecated-1.2.14 dropstackframe-0.1.1 gpflow-2.9.1 lark-1.2.2\n"
          ]
        }
      ],
      "source": [
        "## Colab cell\n",
        "# Mount drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/MyDrive/Research Project\")\n",
        "!pip3 install tensorflow\n",
        "!pip3 install gpflow==2.9.1\n",
        "# !pip3 install sentence-transformers\n",
        "# !pip3 install langchain_text_splitters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvzuDezZr1-s"
      },
      "source": [
        "# Generate embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gr7_b1dRr1xg"
      },
      "outputs": [],
      "source": [
        "# read in data\n",
        "import pandas as pd\n",
        "import ast\n",
        "from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "results_df = pd.read_csv(\"data/mistral_results_run2.csv\")[:7500] # only considering 7500 points\n",
        "\n",
        "# preprocessing of true answer\n",
        "def preprocess_true_answer(row):\n",
        "    '''\n",
        "    Reads in the true answer column correctly. It is written as a numpy array containing a string, as csv reads in numpy as a string itself.\n",
        "    \"['e.g']\" -> 'e.g'\n",
        "    '''\n",
        "    convert_from_string = ast.literal_eval(row[\"true_answer\"])\n",
        "    if len(convert_from_string) == 0:\n",
        "        convert_from_string = [\"Context does not contain the answer.\"]\n",
        "\n",
        "    return convert_from_string[0]\n",
        "\n",
        "def preprocess_llm_answer(row):\n",
        "    llm_answer = row[\"llm_answer\"]\n",
        "    if \"Context does not contain the answer\" in llm_answer:\n",
        "        llm_answer = \"Context does not contain the answer.\"\n",
        "    return llm_answer\n",
        "\n",
        "results_df[\"true_answer\"] = results_df.apply(preprocess_true_answer, axis=1)\n",
        "results_df[\"llm_answer\"] = results_df.apply(preprocess_llm_answer, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhTKAdoEt7EI"
      },
      "outputs": [],
      "source": [
        "# Remove long question+contexts\n",
        "\n",
        "text_splitter = SentenceTransformersTokenTextSplitter(model_name=\"sentence-transformers/all-mpnet-base-v2\") # tokenise according to mpnet-base ST\n",
        "all_questions = results_df[\"question\"].tolist()\n",
        "all_contexts = results_df[\"context\"].tolist()\n",
        "all_question_context_combined = [f\"{question} {context}\" for question,context in zip(all_questions, all_contexts)] # string strategy\n",
        "\n",
        "num_tokens_question_context_combined = [text_splitter.count_tokens(text=entry) - 2 for entry in all_question_context_combined] # count tokens for string strategy\n",
        "indices_too_long = [index for index, token_count in enumerate(num_tokens_question_context_combined) if token_count > 382] # indices that have a token length of > 384 and hence will be truncated\n",
        "\n",
        "results_df = results_df.drop(indices_too_long).reset_index(drop=True)\n",
        "results_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5dEuQKRt8An"
      },
      "outputs": [],
      "source": [
        "text_splitter = SentenceTransformersTokenTextSplitter(model_name=\"sentence-transformers/all-distilroberta-v1\") # tokenise according to distilroberta ST\n",
        "all_questions = results_df[\"question\"].tolist()\n",
        "all_contexts = results_df[\"context\"].tolist()\n",
        "all_question_context_combined = [f\"{question} {context}\" for question,context in zip(all_questions, all_contexts)] # string strategy\n",
        "\n",
        "num_tokens_question_context_combined = [text_splitter.count_tokens(text=entry) - 2 for entry in all_question_context_combined] # count tokens for string strategy\n",
        "indices_too_long = [index for index, token_count in enumerate(num_tokens_question_context_combined) if token_count > 510] # indices that have a token length of > 384 and hence will be truncated\n",
        "results_df = results_df.drop(indices_too_long).reset_index(drop=True)\n",
        "\n",
        "print(results_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbI0TUiBvwGA"
      },
      "outputs": [],
      "source": [
        "def get_embedding(sentences_list, model):\n",
        "    '''\n",
        "    This function produces an embedding for a list of sentences that is provided to the function.\n",
        "    '''\n",
        "    pool = model.start_multi_process_pool()\n",
        "    embeddings = model.encode_multi_process(sentences_list, pool)\n",
        "    model.stop_multi_process_pool(pool)\n",
        "    return embeddings.tolist()\n",
        "\n",
        "## Create input embeddings\n",
        "all_questions = results_df[\"question\"].tolist()\n",
        "all_contexts = results_df[\"context\"].tolist()\n",
        "all_question_context_combined = [f\"{question} {context}\" for question,context in zip(all_questions, all_contexts)]\n",
        "\n",
        "# load sentence transformer model\n",
        "model_list = {\n",
        "    \"mpnet\" : SentenceTransformer(\"all-mpnet-base-v2\"),\n",
        "    \"distill_roberta\" : SentenceTransformer(\"all-distilroberta-v1\")\n",
        "}\n",
        "\n",
        "for input_representation in [\"mpnet\", \"distill_roberta\"]:\n",
        "    print(\"Input \", input_representation)\n",
        "    # string strat\n",
        "    results_df[f\"question_and_context_{input_representation}\"] = get_embedding(all_question_context_combined, model_list[input_representation])\n",
        "\n",
        "    # concat strat\n",
        "    results_df[f\"question_{input_representation}\"] = get_embedding(all_questions, model_list[input_representation])\n",
        "    results_df[f\"context_{input_representation}\"] = get_embedding(all_contexts, model_list[input_representation])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8mlofiVlwrmV"
      },
      "outputs": [],
      "source": [
        "def calc_similarity_score_column(column1, column2, model):\n",
        "    '''\n",
        "    Calculate the similarity scores (pairwise) between the first and second column. (dataset answer and llm answer)\n",
        "    column1 and column2 is list of strings.\n",
        "    '''\n",
        "    column1_embedding = model.encode(column1)\n",
        "    column2_embedding = model.encode(column2)\n",
        "    similarity_scores = model.similarity_pairwise(column1_embedding, column2_embedding)\n",
        "    return similarity_scores.numpy()\n",
        "\n",
        "# calculate targets\n",
        "all_true_answers_list = results_df[\"true_answer\"].tolist()\n",
        "all_llm_answers_list = results_df[\"llm_answer\"].tolist()\n",
        "\n",
        "for target_representation in [\"mpnet\", \"distill_roberta\"]:\n",
        "    results_df[f\"{target_representation}_score\"] = calc_similarity_score_column(all_true_answers_list, all_llm_answers_list, model_list[target_representation])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CYxlwSSWz-0s"
      },
      "outputs": [],
      "source": [
        "results_df.to_csv(\"data/fullsubset_embeddings_multiple_inputs.csv\", index=False) # save to csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzTQReEqr4Cg"
      },
      "source": [
        "# Fit GP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "z03w4yViUYeu"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from gpflow.models import GPR\n",
        "from gpflow.kernels import RationalQuadratic\n",
        "from gpflow.mean_functions import Zero\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import pickle\n",
        "import scipy.stats as stats\n",
        "import gpflow\n",
        "\n",
        "SEED = 2504\n",
        "\n",
        "# read in generated embeddings and targets\n",
        "results_df = pd.read_csv(\"data/fullsubset_embeddings_multiple_inputs.csv\") # Dataset\n",
        "\n",
        "## correctly read in saved embeddings\n",
        "# mpnet embeddings\n",
        "results_df[\"question_mpnet\"] = [ast.literal_eval(x) for x in results_df[\"question_mpnet\"]]\n",
        "results_df[\"context_mpnet\"] = [ast.literal_eval(x) for x in results_df[\"context_mpnet\"]]\n",
        "results_df[\"question_and_context_mpnet\"] = [ast.literal_eval(x) for x in results_df[\"question_and_context_mpnet\"]]\n",
        "\n",
        "# # distill_roberta_embeddings\n",
        "results_df[\"question_distill_roberta\"] = [ast.literal_eval(x) for x in results_df[\"question_distill_roberta\"]]\n",
        "results_df[\"context_distill_roberta\"] = [ast.literal_eval(x) for x in results_df[\"context_distill_roberta\"]]\n",
        "results_df[\"question_and_context_distill_roberta\"] = [ast.literal_eval(x) for x in results_df[\"question_and_context_distill_roberta\"]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwDEmICMtMw4"
      },
      "outputs": [],
      "source": [
        "def preds_vs_truth_plot(train_true_vals, train_pred_vals, test_true_vals, test_pred_vals, file_name=None):\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12,8))\n",
        "\n",
        "    ## plot train data\n",
        "    axs[0].scatter(train_true_vals, train_pred_vals, alpha=0.4, label=f\"Train data\", color=\"blue\") # plot train points\n",
        "\n",
        "    # plot line of perfec predictions for comparison\n",
        "    min_value_train = min(np.min(train_true_vals), np.min(train_pred_vals))\n",
        "    max_value_train = max(np.max(train_true_vals), np.max(train_pred_vals))\n",
        "    axs[0].plot(np.arange(min_value_train, max_value_train, step=0.001),\n",
        "                np.arange(min_value_train, max_value_train, step=0.001),\n",
        "                label=\"Perfect predictions\", color=\"red\")\n",
        "    axs[0].set_xlabel(\"True score\")\n",
        "    axs[0].set_ylabel(\"Predicted score\")\n",
        "    axs[0].legend(fontsize=\"x-small\")\n",
        "    axs[0].set_title(\"Train\", fontsize=10)\n",
        "\n",
        "    ## plot test data\n",
        "    axs[1].scatter(test_true_vals, test_pred_vals, alpha=0.4, label=f\"Train data\", color=\"blue\") # plot test points\n",
        "\n",
        "    # plot line of perfect predictions for comparison\n",
        "    min_val_test = min(np.min(test_true_vals), np.min(test_pred_vals))\n",
        "    max_val_test = max(np.max(test_true_vals), np.max(test_pred_vals))\n",
        "\n",
        "    axs[1].plot(np.arange(min_val_test, max_val_test, step=0.001),\n",
        "                np.arange(min_val_test, max_val_test, step=0.001),\n",
        "                label=\"Perfect predictions\", color=\"red\")\n",
        "    axs[1].set_xlabel(\"True score\")\n",
        "    axs[1].set_ylabel(\"Predicted score\")\n",
        "    axs[1].legend(fontsize=\"x-small\")\n",
        "    axs[1].set_title(\"Test\", fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if file_name is not None:\n",
        "        plt.savefig(file_name)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "D_4t3J6VUpt1"
      },
      "outputs": [],
      "source": [
        "def gp_model_init(x_train, y_train, lengthscale, alpha, noise):\n",
        "    # Initilaise GP with data and hyperparameters provided\n",
        "\n",
        "    return GPR(\n",
        "        data=(x_train, y_train),\n",
        "        kernel= RationalQuadratic(lengthscales=lengthscale, alpha=alpha),\n",
        "        mean_function=Zero(), # zero mean function\n",
        "        noise_variance=noise\n",
        "    )\n",
        "\n",
        "def fit_gp_function(results_df, all_gp_inputs, target, save_folder, num_restarts=20):\n",
        "    best_model, best_log_marginal_likelihood_val = None, -np.inf\n",
        "\n",
        "    all_targets = results_df[target].to_numpy().reshape(-1, 1)\n",
        "\n",
        "    # 80-20 train-test split\n",
        "    x_train, x_test, y_train, y_test= train_test_split(all_gp_inputs, all_targets, test_size=0.20, random_state=SEED)\n",
        "\n",
        "    # scale data\n",
        "    x_scaler = StandardScaler()\n",
        "    x_train_scaled = x_scaler.fit_transform(x_train)\n",
        "    x_test_scaled = x_scaler.transform(x_test)\n",
        "\n",
        "    y_scaler = StandardScaler()\n",
        "    y_train_scaled = y_scaler.fit_transform(y_train)\n",
        "\n",
        "    # multiple restarts\n",
        "    for i in range(num_restarts):\n",
        "        print(f\"Restart {i}\")\n",
        "        # generate random initialisation vals\n",
        "        lengthscale_init = stats.loguniform.rvs(0.01, 100)\n",
        "        alpha_init = stats.loguniform.rvs(0.01, 100)\n",
        "        noise_init = stats.loguniform.rvs(0.01, 100)\n",
        "\n",
        "        # initialise Gp\n",
        "        model = gp_model_init(x_train_scaled, y_train_scaled, lengthscale_init, alpha_init, noise_init)\n",
        "        opt = gpflow.optimizers.Scipy() # create optimizer\n",
        "        opt.minimize(model.training_loss, model.trainable_variables) # perform optimization\n",
        "\n",
        "        log_marginal_likelihood_val = model.log_marginal_likelihood().numpy() # get log marginal likelihood\n",
        "\n",
        "        # update if higher log marginal likelihood found\n",
        "        if log_marginal_likelihood_val > best_log_marginal_likelihood_val:\n",
        "            best_log_marginal_likelihood_val = log_marginal_likelihood_val\n",
        "            best_model = model\n",
        "\n",
        "    # get predictions\n",
        "    mean_train, _ = best_model.predict_y(x_train_scaled)\n",
        "    mean_test, _ = best_model.predict_y(x_test_scaled)\n",
        "\n",
        "    mean_test_rev = y_scaler.inverse_transform(mean_test.numpy()) # reverse transform, interpretability reason\n",
        "\n",
        "    # calculate eval metrics\n",
        "    mae = mean_absolute_error(y_test.squeeze(), mean_test_rev)\n",
        "    rmse = mean_squared_error(y_test.squeeze(), mean_test_rev, squared=False)\n",
        "    r2 = r2_score(y_test.squeeze(), mean_test_rev)\n",
        "\n",
        "    # preds_vs_truth_plot(y_scaler.inverse_transform(y_train_scaled), y_scaler.inverse_transform(mean_train.numpy()),\n",
        "    #                     y_test, mean_test_rev)\n",
        "\n",
        "    # save preds and metrics\n",
        "    train_results_df = pd.DataFrame({\n",
        "        \"true\" : y_scaler.inverse_transform(y_train_scaled).squeeze(),\n",
        "        \"prediction\" : y_scaler.inverse_transform(mean_train.numpy()).squeeze()\n",
        "    })\n",
        "\n",
        "    test_results_df = pd.DataFrame({\n",
        "        \"true\" : y_test.squeeze(),\n",
        "        \"prediction\" : mean_test_rev.squeeze()\n",
        "    })\n",
        "\n",
        "    # train_results_df.to_csv(f\"{save_folder}_train_vals.csv\", index=False)\n",
        "    # test_results_df.to_csv(f\"{save_folder}_test_vals.csv\", index=False)\n",
        "\n",
        "    test_metrics_df = pd.DataFrame({\n",
        "        # \"best_marginal_likelihood\" : [best_log_marginal_likelihood]\n",
        "        \"mae\" : [mae],\n",
        "        \"rmse\" : [rmse],\n",
        "        \"r2\" : [r2]\n",
        "    })\n",
        "\n",
        "    # test_metrics_df.to_csv(f\"{save_folder}_test_metrics.csv\", index=False)\n",
        "    print(test_metrics_df)\n",
        "\n",
        "    # save optimized hyperparam of best model\n",
        "    # param_dict = gpflow.utilities.parameter_dict(best_model)\n",
        "    # with open(f\"{save_folder}_param_dict.pkl\", \"wb\") as f:\n",
        "    #     pickle.dump(param_dict, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lalH87URmBG8"
      },
      "outputs": [],
      "source": [
        "all_input_representation = [\"mpnet\", \"distill_roberta\"]\n",
        "all_targets = [\"mpnet_score\", \"distill_roberta_score\"]\n",
        "\n",
        "for input_representation in all_input_representation:\n",
        "    print(input_representation)\n",
        "    for target_representation in all_targets:\n",
        "        print(input_representation, target_representation)\n",
        "        # String\n",
        "        all_gp_inputs = np.array(results_df[f\"question_and_context_{input_representation}\"].tolist())\n",
        "\n",
        "        fit_gp_function(results_df,\n",
        "                all_gp_inputs,\n",
        "                target = target_representation,\n",
        "                save_folder=f\"experiment_1/{target_representation}/input_{input_representation}_string\"\n",
        "                )\n",
        "\n",
        "        # Concat\n",
        "        all_gp_inputs = np.hstack([\n",
        "            np.array(results_df[f\"question_{input_representation}\"].tolist()),\n",
        "            np.array(results_df[f\"context_{input_representation}\"].tolist()),\n",
        "            ])\n",
        "\n",
        "        fit_gp_function(results_df,\n",
        "                all_gp_inputs,\n",
        "                target = target_representation,\n",
        "                save_folder=f\"experiment_1/{target_representation}/input_{input_representation}_concat\"\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ITvZNjk9bwm"
      },
      "outputs": [],
      "source": [
        "# dummy regressor (predicts mean)\n",
        "from sklearn.dummy import DummyRegressor\n",
        "input_representation = \"mpnet\"\n",
        "target_representation = \"mpnet_score\"\n",
        "all_targets = results_df[target_representation].to_numpy().reshape(-1, 1)\n",
        "\n",
        "all_gp_inputs = np.hstack([\n",
        "            np.array(results_df[f\"question_{input_representation}\"].tolist()),\n",
        "            np.array(results_df[f\"context_{input_representation}\"].tolist()),\n",
        "            ])\n",
        "\n",
        "# 80-20 train-test split\n",
        "x_train, x_test, y_train, y_test= train_test_split(all_gp_inputs, all_targets, test_size=0.20, random_state=SEED)\n",
        "\n",
        "dummy_reg = DummyRegressor(strategy=\"mean\")\n",
        "dummy_reg.fit(x_train, y_train.ravel())\n",
        "\n",
        "mean_test = dummy_reg.predict(x_test).reshape(-1,1)\n",
        "\n",
        "# calculate eval metrics\n",
        "mae = mean_absolute_error(y_test.squeeze(), mean_test)\n",
        "rmse = mean_squared_error(y_test.squeeze(), mean_test, squared=False)\n",
        "r2 = r2_score(y_test.squeeze(), mean_test)\n",
        "\n",
        "print(f\"MAE : {mae}\")\n",
        "print(f\"RMSE : {rmse}\")\n",
        "print(f\"R2 : {r2}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
